{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GEE Zonal","text":"<p>This python package provides a wrapper function to request temporal and zonal statistics from Google Earth Engine (GEE) datasets.</p>"},{"location":"#summary","title":"Summary","text":"<p>A zonal statistics function was created to ease the process of working with the GEE API. The function can work with any raster data loaded to EE (<code>ee.ImageCollections</code> or <code>ee.Image</code>) and vector features (<code>geopandas.GeoDataFrame</code> or <code>ee.FeatureCollection</code>), and returns tabular data.</p> <p>Statistics can be requested at various temporal resolutions (<code>original</code> frequency, <code>monthly</code>, or <code>annual</code>). The workflow conducts pixel-by-pixel temporal aggregations, before summarizing statistics over target features.</p> <p>Additionally, the package provides functionality to quickly search the GEE Catalog.</p> <p>Note</p> <p>This project is under active development. Feel free to open a new issue or a discussion if you have any questions or suggestions.</p>"},{"location":"installation/","title":"Installation","text":"<p>The required dependencies are earthengine-api, geopandas, geojson, and notebook. The package (and dependencies) can be installed via pip:</p> <pre><code>pip install gee_zonal\n</code></pre> <p>This is the preferred method to install geodev, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#setup","title":"Setup","text":"<p>The Earth Engine Python API needs to be authenticated with a Google account. First, sign up to Google Earth Engine here. You will also need to create a Google Cloud Project.</p> <p>Launch a jupyter notebook, and authenticate your account with the ee library. You will be able to create a project after running the authenticate command, detailed instructions are available here</p> <pre><code>import ee\nee.Authenticate()\n</code></pre> <p>Note</p> <p>Authenticating from within a terminal can lead to issues with gcloud.</p> <p>You can check that this worked by running <code>ee.Initialize()</code>, then import and run the library:</p> <pre><code>from gee_zonal import ZonalStats, Catalog\n</code></pre> <p>If the pip installation is not working, you can install the package from source:</p> <pre><code>pip install git+https://github.com/worldbank/GEE_Zonal.git\n</code></pre>"},{"location":"pydrive/","title":"PyDrive Support","text":"<p>As demonstrated in the example notebook, zonal statistics can be submitted as tasks using Earth Engine's <code>ee.batch.Export.table.toDrive</code> feature. This is generally preferred when running large computations, or trying to batch several tasks in parallel.</p> <p>The <code>ZonalStats</code> object contains a function <code>getZonalStats()</code> to retrieve the resulting table directly from Google Drive using pydrive.</p> <p>This requires additional configuration to provide access to Google Drive. First, the pydrive package needs to be installed.</p> <pre><code>pip install pydrive\n</code></pre>"},{"location":"pydrive/#setup","title":"Setup","text":"<p>Follow the instructions to obtain a client configuration file <code>client_secrets.json</code>.</p> <p>One additional step that you need to do is go to Audience and add your email as a test user. Otherwise, you will have to publish the application.</p> <p>Place the <code>client_secrets.json</code> file in a folder such that the <code>authenticateGoogleDrive()</code> function can find it. By default the function looks for the file in <code>~/.config/creds/client_secrets.json</code>. You can change this by passing the <code>creds_dir</code> argument to the function.</p>"},{"location":"pydrive/#usage","title":"Usage","text":"<pre><code>from gee_zonal import authenticateGoogleDrive\nfrom os.path.import join\n\n# Authorize Google Drive\ndrive = authenticateGoogleDrive(creds_dir = join(expanduser('~'), '.config', 'creds')) # Change to path where you stored client_secrets file\n\n# Run ZonalStats\nzs = ZonalStats(\n    collection_id='UCSB-CHG/CHIRPS/PENTAD',\n    target_features=AOIs,\n    statistic_type=\"mean\",\n    temporal_stat=\"sum\",\n    frequency=\"annual\",\n    scale=5000,\n    output_dir = \"gdrive_folder\",\n    output_name=\"pretty_output\"\n)\n\n# Submit task to Earth Engine\nzs.runZonalStats()\n\n# Check if task is completed\nzs.reportRunTime()\n\n# Retrieve result\ndf = zs.getZonalStats(drive)\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#creating-a-zonalstats-object","title":"Creating a ZonalStats object","text":"<p>ZonalStats() is the main class to request temporal and zonal statistics using the GEE backend. The object can be initialized with parameters specifying data inputs and the type of aggregation.</p> <p>Input target features can be referenced directly as a GEE asset, or can be supplied as a <code>geopandas.GeoDataFrame</code>, or a path to a shapefile/GeoJSON (will be automatically converted to <code>ee.FeatureCollection</code>).</p>"},{"location":"usage/#gee_zonal.zonalstats.ZonalStats","title":"<code>gee_zonal.zonalstats.ZonalStats</code>","text":"<p>               Bases: <code>object</code></p> <p>Python class to calculate zonal and temporal statistics from Earth Engine datasets (ee.ImageCollection or ee.Image) over vector shapes (ee.FeatureCollections).</p> <p>Parameters:</p> Name Type Description Default <code>target_features</code> <code>ee.FeatureCollection | gpd.GeoDataFrame | str (path to a shapefile/GeoJSON)</code> <p>vector features</p> required <code>statistic_type</code> <code>str (mean, max, median, min, sum, stddev, var, count, minmax, p75, p25, p95, all)</code> <p>method to aggregate image pixels by zone</p> required <code>collection_id</code> <code>str</code> <p>ID for Earth Engine dataset</p> <code>None</code> <code>ee_dataset</code> <code>ee.Image | ee.ImageCollection</code> <p>input dataset if no collection ID is provided</p> <code>None</code> <code>band</code> <code>str</code> <p>name of image band to use</p> <code>None</code> <code>output_name</code> <code>str</code> <p>file name for output statistics if saved to Google Drive</p> <code>None</code> <code>output_dir</code> <code>str</code> <p>directory name for output statistics if saved to Google Drive</p> <code>None</code> <code>frequency</code> <code>str (monthly | annual | original)</code> <p>temporal frequency for aggregation</p> <code>'original'</code> <code>temporal_stat</code> <code>str (mean, max, median, min, sum)</code> <p>statistic for temporal aggregation</p> <code>None</code> <code>scale</code> <code>int</code> <p>scale for calculation in mts</p> <code>250</code> <code>min_threshold</code> <code>int</code> <p>filter out values lower than threshold</p> <code>None</code> <code>mask</code> <code>ee.Image</code> <p>filter out observations where mask is zero</p> <code>None</code> <code>tile_scale</code> <code>int</code> <p>tile scale factor for parallel processing</p> <code>1</code> <code>start_year</code> <code>int</code> <p>specify start year for statistics</p> <code>None</code> <code>end_year</code> <code>int</code> <p>specify end year for statistics</p> <code>None</code> <code>scale_factor</code> <code>int</code> <p>scale factor to multiply ee.Image to get correct units</p> <code>None</code> <code>mapped</code> <code>bool</code> <p>Boolean to indicate whether to use mapped or non-mapped version of zonal stats</p> <code>False</code> Source code in <code>gee_zonal/zonalstats.py</code> <pre><code>class ZonalStats(object):\n    \"\"\"\n    Python class to calculate zonal and temporal statistics from Earth Engine datasets (ee.ImageCollection or ee.Image) over vector shapes (ee.FeatureCollections).\n\n    :param target_features: vector features\n    :type target_features: ee.FeatureCollection or gpd.GeoDataFrame or str (path to a shapefile/GeoJSON)\n    :param statistic_type: method to aggregate image pixels by zone\n    :type statistic_type: str (mean, max, median, min, sum, stddev, var, count, minmax, p75, p25, p95, all)\n    :param collection_id: ID for Earth Engine dataset\n    :type collection_id: str\n    :param ee_dataset: input dataset if no collection ID is provided\n    :type ee_dataset: ee.Image or ee.ImageCollection\n    :param band: name of image band to use\n    :type band: str\n    :param output_name: file name for output statistics if saved to Google Drive\n    :type output_name: str\n    :param output_dir: directory name for output statistics if saved to Google Drive\n    :type output_dir: str\n    :param frequency: temporal frequency for aggregation\n    :type frequency: str (monthly or annual or original)\n    :param temporal_stat: statistic for temporal aggregation\n    :type temporal_stat: str (mean, max, median, min, sum)\n    :param scale: scale for calculation in mts\n    :type scale: int\n    :param min_threshold: filter out values lower than threshold\n    :type min_threshold: int\n    :param mask: filter out observations where mask is zero\n    :type mask: ee.Image\n    :param tile_scale: tile scale factor for parallel processing\n    :type tile_scale: int\n    :param start_year: specify start year for statistics\n    :type start_year: int\n    :param end_year: specify end year for statistics\n    :type end_year: int\n    :param scale_factor: scale factor to multiply ee.Image to get correct units\n    :type scale_factor: int\n    :param mapped: Boolean to indicate whether to use mapped or non-mapped version of zonal stats\n    :type mapped: bool\n    \"\"\"\n\n    def __init__(\n        self,\n        target_features,\n        statistic_type,\n        collection_id=None,\n        ee_dataset=None,\n        band=None,\n        output_name=None,\n        output_dir=None,\n        frequency=\"original\",\n        temporal_stat=None,\n        scale=250,\n        min_threshold=None,\n        mask=None,\n        tile_scale=1,\n        start_year=None,\n        end_year=None,\n        scale_factor=None,\n        mapped=False,\n    ):\n        self.collection_id = collection_id\n        if collection_id is None and ee_dataset is None:\n            raise Exception(\"One of collection_id or ee_dataset must be supplied\")\n        self.collection_suffix = (\n            collection_id[collection_id.rfind(\"/\") + 1 :] if collection_id else None\n        )\n        if ee_dataset is None:\n            try:\n                ee.ImageCollection(collection_id).getInfo()\n                self.ee_dataset = (\n                    ee.ImageCollection(collection_id)\n                    if band is None\n                    else ee.ImageCollection(collection_id).select(band)\n                )\n            except Exception:\n                try:\n                    ee.Image(collection_id).getInfo()\n                    self.ee_dataset = (\n                        ee.Image(collection_id)\n                        if band is None\n                        else ee.Image(collection_id).select(band)\n                    )\n                except Exception:\n                    raise Exception(\"Collection ID does not exist\")\n        else:\n            self.ee_dataset = ee_dataset\n        cat = Catalog()\n        self.metadata = (\n            cat.datasets.loc[cat.datasets.id == collection_id].iloc[0]\n            if collection_id\n            else None\n        )\n        self.target_features = (\n            target_features\n            if type(target_features) is ee.FeatureCollection\n            else gpd_to_gee(target_features)\n        )\n        self.statistic_type = statistic_type\n        self.frequency = frequency\n        self.temporal_stat = temporal_stat\n        self.output_dir = output_dir\n        self.output_name = output_name\n        self.task = None\n        self.scale = scale\n        self.min_threshold = min_threshold\n        self.mask = mask\n        self.scale_factor = scale_factor\n        self.tile_scale = tile_scale\n        self.start_year = start_year\n        self.end_year = end_year\n        self.mapped = mapped\n\n    def yList(self, start=None, end=None):\n        \"\"\"\n        Create list of years from a given dataset\n        \"\"\"\n        if start is None:\n            years = list(range(self.metadata.startyear, self.metadata.endyear + 1, 1))\n        else:\n            years = list(range(start, end + 1, 1))\n        return ee.List(years)\n\n    def ymList(self, start=None, end=None):\n        \"\"\"\n        Create list of year/month pairs from a given dataset\n        \"\"\"\n        if start is None and end is None:\n            start = self.metadata.start_date\n            end = self.metadata.end_date\n            ym_range = pd.date_range(\n                datetime(start.year, start.month, 1),\n                datetime(end.year, end.month, 1),\n                freq=\"MS\",\n            )\n            ym_range = list(date.strftime(\"%Y%m\") for date in ym_range)\n        else:\n            ym_range = pd.date_range(\n                datetime(start, 1, 1), datetime(end, 12, 31), freq=\"MS\"\n            )\n            ym_range = list(date.strftime(\"%Y%m\") for date in ym_range)\n        return ee.List(ym_range)\n\n    def ymList_ee(self):\n        \"\"\"\n        Create list of year/month pairs from a given dataset using EE\n        \"\"\"\n\n        def iter_func(image, newlist):\n            date = ee.Number.parse(image.date().format(\"YYYYMM\")).format()\n            newlist = ee.List(newlist)\n            return ee.List(newlist.add(date).sort())\n\n        ymd = self.ee_dataset.iterate(iter_func, ee.List([]))\n        return ee.List(ymd).distinct()\n\n    def temporalStack(self, date_list, freq, stat):\n        allowed_statistics_ts = {\n            \"mean\": ee.Reducer.mean(),\n            \"max\": ee.Reducer.max(),\n            \"median\": ee.Reducer.median(),\n            \"min\": ee.Reducer.min(),\n            \"sum\": ee.Reducer.sum(),\n            \"stddev\": ee.Reducer.stdDev(),\n        }\n        if stat not in allowed_statistics_ts.keys():\n            raise Exception(\n                \"temporal statistic must be one of be one of {}\".format(\n                    \", \".join(list(allowed_statistics_ts.keys()))\n                )\n            )\n\n        def aggregate_monthly(ym):\n            date = ee.Date.parse(\"YYYYMM\", ym)\n            y = date.get(\"year\")\n            m = date.get(\"month\")\n            monthly = (\n                self.ee_dataset.filter(ee.Filter.calendarRange(y, y, \"year\"))\n                .filter(ee.Filter.calendarRange(m, m, \"month\"))\n                .reduce(allowed_statistics_ts[stat])\n                .set(\"month\", m)\n                .set(\"year\", y)\n                .set(\"system:index\", ee.String(y.format().cat(\"_\").cat(m.format())))\n            )\n            return monthly\n\n        def aggregate_annual(y):\n            y = ee.Number(y)\n            annual = (\n                self.ee_dataset.filter(ee.Filter.calendarRange(y, y, \"year\"))\n                .reduce(allowed_statistics_ts[stat])\n                .set(\"year\", y)\n                .set(\"system:index\", ee.String(y.format()))\n            )\n            return annual\n\n        if freq == \"monthly\":\n            byTime = ee.ImageCollection.fromImages(date_list.map(aggregate_monthly))\n        if freq == \"annual\":\n            byTime = ee.ImageCollection.fromImages(date_list.map(aggregate_annual))\n        return byTime  # .toBands()\n\n    def applyWaterMask(self, image, year=None):\n        land_mask = (\n            ee.Image(\"MODIS/MOD44W/MOD44W_005_2000_02_24\").select(\"water_mask\").eq(0)\n        )\n        return image.updateMask(land_mask)\n\n    def applyMinThreshold(self, image, min_threshold):\n        bool_mask = image.gte(min_threshold)\n        return image.updateMask(bool_mask)\n\n    def applyMask(self, image, mask):\n        return image.updateMask(mask)\n\n    def applyScaleFactor(self, image, scale_factor):\n        return image.multiply(scale_factor)\n\n    def runZonalStats(self):\n        \"\"\"\n        Run zonal statistics aggregation\n\n        :return: tabular statistics\n        :rtype: DataFrame or dict with EE task status if output_name/dir is specified\n        \"\"\"\n        if self.frequency not in [\"monthly\", \"annual\", \"original\"]:\n            raise Exception(\"frequency must be one of annual, monthly, or original\")\n        if self.frequency == \"monthly\":\n            timesteps = self.ymList(self.start_year, self.end_year)\n        elif self.frequency == \"annual\":\n            timesteps = self.yList(self.start_year, self.end_year)\n        elif self.frequency == \"original\":\n            if self.start_year is not None and self.end_year is not None:\n                start_year_format = datetime(self.start_year, 1, 1).strftime(\"%Y-%m-%d\")\n                end_year_format = datetime(self.end_year, 12, 31).strftime(\"%Y-%m-%d\")\n                self.ee_dataset = self.ee_dataset.filterDate(\n                    start_year_format, end_year_format\n                )\n        # byTimesteps = self.ee_dataset.toBands() if self.frequency==\"original\" else self.temporalStack(timesteps, self.frequency, self.temporal_stat)\n        if self.frequency == \"original\":\n            if type(self.ee_dataset) is ee.image.Image:\n                byTimesteps = self.ee_dataset\n            elif type(self.ee_dataset) is ee.imagecollection.ImageCollection:\n                byTimesteps = self.ee_dataset.toBands()\n        else:\n            byTimesteps = self.temporalStack(\n                timesteps, self.frequency, self.temporal_stat\n            )\n            byTimesteps = byTimesteps.toBands()\n\n        # pre-processing\n        if self.mask is not None:\n            if self.mask == \"water\":\n                byTimesteps = self.applyWaterMask(byTimesteps)\n            elif type(self.mask) is ee.image.Image:\n                byTimesteps = self.applyMask(byTimesteps, self.mask)\n        if self.min_threshold is not None:\n            byTimesteps = self.applyMinThreshold(byTimesteps, self.min_threshold)\n        if self.scale_factor is not None:\n            byTimesteps = self.applyScaleFactor(byTimesteps, self.scale_factor)\n\n        allowed_statistics = {\n            \"count\": ee.Reducer.frequencyHistogram().unweighted(),\n            \"mean\": ee.Reducer.mean(),\n            \"max\": ee.Reducer.max(),\n            \"median\": ee.Reducer.median(),\n            \"min\": ee.Reducer.min(),\n            \"sum\": ee.Reducer.sum(),\n            \"stddev\": ee.Reducer.stdDev(),\n            \"var\": ee.Reducer.variance(),\n            \"minmax\": ee.Reducer.minMax(),\n            \"p75\": ee.Reducer.percentile(\n                [75]\n            ),  # maxBuckets=10 , minBucketWidth=1, maxRaw=1000\n            \"p25\": ee.Reducer.percentile(\n                [25]\n            ),  # maxBuckets=10 , minBucketWidth=1, maxRaw=1000\n            \"p95\": ee.Reducer.percentile(\n                [95]\n            ),  # maxBuckets=10 , minBucketWidth=1, maxRaw=1000\n            \"all\": ee.Reducer.mean()\n            .combine(ee.Reducer.minMax(), sharedInputs=True)\n            .combine(ee.Reducer.stdDev(), sharedInputs=True),\n        }\n\n        def combine_reducers(reducer_list):\n            for i, r in enumerate(reducer_list):\n                if i == 0:\n                    reducer = r\n                if i &gt; 0:\n                    reducer = reducer.combine(r, sharedInputs=True)\n            return reducer\n\n        if type(self.statistic_type) is str:\n            if self.statistic_type not in allowed_statistics.keys():\n                raise Exception(\n                    \"statistic must be one of be one of {}\".format(\n                        \", \".join(list(allowed_statistics.keys()))\n                    )\n                )\n            else:\n                reducer = allowed_statistics[self.statistic_type]\n        elif type(self.statistic_type) is list:\n            for stat_type in self.statistic_type:\n                if stat_type not in allowed_statistics.keys():\n                    raise Exception(\n                        \"statistic must be one of be one of {}\".format(\n                            \", \".join(list(allowed_statistics.keys()))\n                        )\n                    )\n            reducer_list = [\n                allowed_statistics[stat_type] for stat_type in self.statistic_type\n            ]\n            reducer = combine_reducers(reducer_list)\n\n        if self.mapped:\n\n            def zs_func(feature):\n                zs_result = byTimesteps.reduceRegion(\n                    reducer=reducer,\n                    geometry=feature.geometry(),\n                    scale=self.scale,\n                    maxPixels=10e15,  # 1e13\n                    tileScale=self.tile_scale,\n                )\n                feature = feature.set(zs_result)\n                return feature\n\n            zs = self.target_features.map(zs_func)  # .getInfo()\n        else:\n            zs = ee.Image(byTimesteps).reduceRegions(\n                collection=self.target_features,\n                reducer=reducer,\n                scale=self.scale,\n                tileScale=self.tile_scale,\n            )\n        if self.output_dir is not None and self.output_name is not None:\n            self.task = ee.batch.Export.table.toDrive(\n                collection=zs,\n                description=f\"Zonal statistics for {self.collection_suffix}\",\n                fileFormat=\"CSV\",\n                folder=self.output_dir,\n                fileNamePrefix=self.output_name,\n            )\n            self.task.start()\n            # return(self)\n        else:\n            res = zs.getInfo()\n            return self.get_zonal_res(res)\n\n    def get_zonal_res(self, res, rename=None):\n        \"\"\"\n        Create a data frame from the results of GEE zonal\n        :param res: response from RunZonalStats method retrieved via featureCollection.getInfo()\n        :type res: dictionary from ee.FeatureCollection\n        \"\"\"\n        feats = res[\"features\"]\n        ids = [f[\"id\"] for f in feats]\n        series = [pd.Series(f[\"properties\"]) for f in feats]\n        df = pd.DataFrame(data=series, index=ids)\n        if rename:\n            df.rename(columns=rename, inplace=True)\n        return df\n\n    def reportRunTime(self):\n        start_time = self.task.status()[\"start_timestamp_ms\"]\n        update_time = self.task.status()[\"update_timestamp_ms\"]\n        if self.task.status()[\"state\"] == \"RUNNING\":\n            delta = datetime.now() - datetime.fromtimestamp(start_time / 1000)\n            print(\"Still running\")\n            print(\n                f\"Runtime: {delta.seconds//60} minutes and {delta.seconds % 60} seconds\"\n            )\n        if self.task.status()[\"state\"] == \"COMPLETED\":\n            delta = datetime.fromtimestamp(update_time / 1000) - datetime.fromtimestamp(\n                start_time / 1000\n            )\n            print(\"Completed\")\n            print(\n                f\"Runtime: {delta.seconds//60} minutes and {delta.seconds % 60} seconds\"\n            )\n        if self.task.status()[\"state\"] == \"FAILED\":\n            print(\"Failed!\")\n            print(self.task.status()[\"error_message\"])\n        if self.task.status()[\"state\"] == \"READY\":\n            print(\"Status is Ready, hasn't started\")\n\n    def getZonalStats(self, drive):\n        folder = drive.ListFile(\n            {\n                \"q\": f\"title = '{self.output_dir}' and trashed=false and mimeType = 'application/vnd.google-apps.folder'\"\n            }\n        ).GetList()[0]\n        folder_id = folder[\"id\"]\n        export_file = drive.ListFile(\n            {\n                \"q\": f\"'{folder_id}' in parents and trashed=false and title contains '{self.output_name}'\"\n            }\n        ).GetList()[0]\n        s = export_file.GetContentString()\n        c = pd.read_csv(io.StringIO(s))\n        c.drop([\".geo\", \"system:index\"], axis=1, inplace=True)\n        return c\n</code></pre>"},{"location":"usage/#gee_zonal.zonalstats.ZonalStats.runZonalStats","title":"<code>gee_zonal.zonalstats.ZonalStats.runZonalStats()</code>","text":"<p>Run zonal statistics aggregation</p> <p>Returns:</p> Type Description <code>DataFrame | dict with EE task status if output_name/dir is specified</code> <p>tabular statistics</p> Source code in <code>gee_zonal/zonalstats.py</code> <pre><code>def runZonalStats(self):\n    \"\"\"\n    Run zonal statistics aggregation\n\n    :return: tabular statistics\n    :rtype: DataFrame or dict with EE task status if output_name/dir is specified\n    \"\"\"\n    if self.frequency not in [\"monthly\", \"annual\", \"original\"]:\n        raise Exception(\"frequency must be one of annual, monthly, or original\")\n    if self.frequency == \"monthly\":\n        timesteps = self.ymList(self.start_year, self.end_year)\n    elif self.frequency == \"annual\":\n        timesteps = self.yList(self.start_year, self.end_year)\n    elif self.frequency == \"original\":\n        if self.start_year is not None and self.end_year is not None:\n            start_year_format = datetime(self.start_year, 1, 1).strftime(\"%Y-%m-%d\")\n            end_year_format = datetime(self.end_year, 12, 31).strftime(\"%Y-%m-%d\")\n            self.ee_dataset = self.ee_dataset.filterDate(\n                start_year_format, end_year_format\n            )\n    # byTimesteps = self.ee_dataset.toBands() if self.frequency==\"original\" else self.temporalStack(timesteps, self.frequency, self.temporal_stat)\n    if self.frequency == \"original\":\n        if type(self.ee_dataset) is ee.image.Image:\n            byTimesteps = self.ee_dataset\n        elif type(self.ee_dataset) is ee.imagecollection.ImageCollection:\n            byTimesteps = self.ee_dataset.toBands()\n    else:\n        byTimesteps = self.temporalStack(\n            timesteps, self.frequency, self.temporal_stat\n        )\n        byTimesteps = byTimesteps.toBands()\n\n    # pre-processing\n    if self.mask is not None:\n        if self.mask == \"water\":\n            byTimesteps = self.applyWaterMask(byTimesteps)\n        elif type(self.mask) is ee.image.Image:\n            byTimesteps = self.applyMask(byTimesteps, self.mask)\n    if self.min_threshold is not None:\n        byTimesteps = self.applyMinThreshold(byTimesteps, self.min_threshold)\n    if self.scale_factor is not None:\n        byTimesteps = self.applyScaleFactor(byTimesteps, self.scale_factor)\n\n    allowed_statistics = {\n        \"count\": ee.Reducer.frequencyHistogram().unweighted(),\n        \"mean\": ee.Reducer.mean(),\n        \"max\": ee.Reducer.max(),\n        \"median\": ee.Reducer.median(),\n        \"min\": ee.Reducer.min(),\n        \"sum\": ee.Reducer.sum(),\n        \"stddev\": ee.Reducer.stdDev(),\n        \"var\": ee.Reducer.variance(),\n        \"minmax\": ee.Reducer.minMax(),\n        \"p75\": ee.Reducer.percentile(\n            [75]\n        ),  # maxBuckets=10 , minBucketWidth=1, maxRaw=1000\n        \"p25\": ee.Reducer.percentile(\n            [25]\n        ),  # maxBuckets=10 , minBucketWidth=1, maxRaw=1000\n        \"p95\": ee.Reducer.percentile(\n            [95]\n        ),  # maxBuckets=10 , minBucketWidth=1, maxRaw=1000\n        \"all\": ee.Reducer.mean()\n        .combine(ee.Reducer.minMax(), sharedInputs=True)\n        .combine(ee.Reducer.stdDev(), sharedInputs=True),\n    }\n\n    def combine_reducers(reducer_list):\n        for i, r in enumerate(reducer_list):\n            if i == 0:\n                reducer = r\n            if i &gt; 0:\n                reducer = reducer.combine(r, sharedInputs=True)\n        return reducer\n\n    if type(self.statistic_type) is str:\n        if self.statistic_type not in allowed_statistics.keys():\n            raise Exception(\n                \"statistic must be one of be one of {}\".format(\n                    \", \".join(list(allowed_statistics.keys()))\n                )\n            )\n        else:\n            reducer = allowed_statistics[self.statistic_type]\n    elif type(self.statistic_type) is list:\n        for stat_type in self.statistic_type:\n            if stat_type not in allowed_statistics.keys():\n                raise Exception(\n                    \"statistic must be one of be one of {}\".format(\n                        \", \".join(list(allowed_statistics.keys()))\n                    )\n                )\n        reducer_list = [\n            allowed_statistics[stat_type] for stat_type in self.statistic_type\n        ]\n        reducer = combine_reducers(reducer_list)\n\n    if self.mapped:\n\n        def zs_func(feature):\n            zs_result = byTimesteps.reduceRegion(\n                reducer=reducer,\n                geometry=feature.geometry(),\n                scale=self.scale,\n                maxPixels=10e15,  # 1e13\n                tileScale=self.tile_scale,\n            )\n            feature = feature.set(zs_result)\n            return feature\n\n        zs = self.target_features.map(zs_func)  # .getInfo()\n    else:\n        zs = ee.Image(byTimesteps).reduceRegions(\n            collection=self.target_features,\n            reducer=reducer,\n            scale=self.scale,\n            tileScale=self.tile_scale,\n        )\n    if self.output_dir is not None and self.output_name is not None:\n        self.task = ee.batch.Export.table.toDrive(\n            collection=zs,\n            description=f\"Zonal statistics for {self.collection_suffix}\",\n            fileFormat=\"CSV\",\n            folder=self.output_dir,\n            fileNamePrefix=self.output_name,\n        )\n        self.task.start()\n        # return(self)\n    else:\n        res = zs.getInfo()\n        return self.get_zonal_res(res)\n</code></pre>"},{"location":"usage/#retrieving-output-table","title":"Retrieving output table","text":"<ol> <li>Retrieve output table directly</li> </ol> <p>Statistics can be accessed as the result of <code>ZonalStats.runZonalStats()</code>. This will be computed within the python earth engine environment.</p> <pre><code>from gee_zonal import ZonalStats\nAOIs = ee.FeatureCollection('users/afche18/Ethiopia_AOI') # ID of ee.FeatureCollection\nzs = ZonalStats(\n    collection_id = 'LANDSAT/LC08/C01/T1_8DAY_NDVI',\n    target_features = AOIs,\n    statistic_type = \"all\", # all includes min, max, mean, and stddev\n    frequency = \"annual\",\n    temporal_stat = \"mean\"\n)\ndf = zs.runZonalStats()\ndf\n</code></pre> <ol> <li>Submit an EE Task</li> </ol> <p>Alternatively, a task can be submitted to the Earth Engine servers by specifying an output_name and outuput_dir.</p> <p>This option is recommended to run statistics for big areas or for a high number of collections. The output table will be saved on the specified directory in Google Drive.</p> <pre><code>import ee\nfrom gee_tools import ZonalStats\nzs = ZonalStats(\n    collection_id='UCSB-CHG/CHIRPS/PENTAD',\n    target_features=AOIs,\n    statistic_type=\"mean\",\n    temporal_stat=\"sum\",\n    frequency=\"annual\",\n    scale=5000,\n    output_dir = \"gdrive_folder\",\n    output_name=\"pretty_output\"\n)\nzs.runZonalStats()\n</code></pre> <p>The status of the task can be monitored with <code>ZonalStats.reportRunTime()</code></p> <pre><code>zs.reportRunTime()\n&gt;&gt;&gt; Completed\n&gt;&gt;&gt; Runtime: 1 minutes and 31 seconds\n</code></pre>"},{"location":"usage/#searching-the-ee-catalog","title":"Searching the EE catalog","text":"<p>The <code>Earth Engine Data Catalog &lt;https://developers.google.com/earth-engine/datasets&gt;</code> is an archive of public datasets available via Google Earth Engine. The Catalog() class provides a quick way to search for datasets by tags, title, and year / time period.</p>"},{"location":"usage/#initialize-catalog-object","title":"Initialize Catalog Object","text":"<p>The catalog object contains a <code>datasets</code> variable, a <code>DataFrame</code> containing a copy of the Earth Engine data catalog.</p> <pre><code>from gee_zonal import Catalog\ncat = Catalog()\ncat.datasets\n</code></pre>"},{"location":"usage/#search-functions","title":"Search functions","text":"<pre><code>results = cat.search_tags(\"ndvi\")\nresults = results.search_by_period(1985, 2021)\nresults = results.search_title(\"landsat\")\nprint(results)\n</code></pre>"},{"location":"usage/#gee_zonal.catalog.Catalog","title":"<code>gee_zonal.catalog.Catalog</code>","text":"<p>               Bases: <code>object</code></p> <p>Inventory of Earth Engine public, saved as a DataFrame under datasets variable</p> Source code in <code>gee_zonal/catalog.py</code> <pre><code>class Catalog(object):\n    \"\"\"\n    Inventory of Earth Engine public, saved as a DataFrame under datasets variable\n    \"\"\"\n\n    def __init__(self, datasets=None, redownload=False):\n        def load_datasets():\n            if redownload:\n                datasets = pd.read_csv(\n                    \"https://raw.githubusercontent.com/samapriya/Earth-Engine-Datasets-List/master/gee_catalog.csv\"\n                )\n                datasets = datasets[\n                    [\n                        \"id\",\n                        \"provider\",\n                        \"title\",\n                        \"start_date\",\n                        \"end_date\",\n                        \"startyear\",\n                        \"endyear\",\n                        \"type\",\n                        \"tags\",\n                        \"asset_url\",\n                        \"thumbnail_url\",\n                    ]\n                ]\n                datasets.to_csv(\n                    os.path.join(repo_dir, \"Earth-Engine-Datasets-List/eed_latest.csv\"),\n                    index=False,\n                )\n            else:\n                try:\n                    datasets = pd.read_csv(\n                        \"https://raw.githubusercontent.com/samapriya/Earth-Engine-Datasets-List/master/gee_catalog.csv\"\n                    )\n                except Exception:\n                    datasets = pd.read_csv(\n                        os.path.join(\n                            repo_dir, \"Earth-Engine-Datasets-List/eed_latest.csv\"\n                        )\n                    )\n            datasets[\"tags\"] = datasets.tags.apply(lambda x: x.lower())\n            datasets[\"tags\"] = datasets.tags.apply(lambda x: x.split(\", \"))\n            datasets[\"start_date\"] = pd.to_datetime(datasets.start_date)\n            datasets[\"end_date\"] = pd.to_datetime(datasets.end_date)\n            return datasets\n\n        self.datasets = load_datasets() if datasets is None else datasets\n\n    def __str__(self):\n        return self.datasets.title.to_string()\n\n    def __len__(self):\n        return len(self.datasets)\n\n    def search_tags(self, keyword):\n        \"\"\"\n        search for keyword in tags\n        \"\"\"\n        keyword = keyword.lower()\n        search_results = self.datasets.loc[\n            self.datasets.tags.apply(lambda x: keyword in x)\n        ]\n        if len(search_results) &gt; 0:\n            return Catalog(search_results)\n        else:\n            raise Exception(\"No hits!\")\n\n    def search_title(self, keyword):\n        \"\"\"\n        search for keyword in title\n        \"\"\"\n\n        def search_function(title, keyword):\n            match = re.search(keyword, title, flags=re.IGNORECASE)\n            return True if match else False\n\n        search_results = self.datasets.loc[\n            self.datasets.title.apply(search_function, args=[keyword])\n        ]\n        if len(search_results) &gt; 0:\n            return Catalog(search_results)\n        else:\n            raise Exception(\"No hits!\")\n\n    def search_by_year(self, year):\n        \"\"\"\n        get all datasets from a particular year:\n            dataset start &lt;= year &lt;= dataset end\n        \"\"\"\n        search_results = self.datasets.loc[\n            (self.datasets.startyear &lt;= year) &amp; (self.datasets.endyear &gt;= year)\n        ]\n        if len(search_results) &gt; 0:\n            return Catalog(search_results)\n        else:\n            raise Exception(\"No hits!\")\n\n    def search_by_period(self, start, end):\n        \"\"\"\n        get all datasets that intersect a time period:\n            start of dataset &lt;= end year\n            end of dataset &gt;= start year\n        \"\"\"\n        search_results = self.datasets.loc[\n            (self.datasets.startyear &lt;= end) &amp; (self.datasets.endyear &gt;= start)\n        ]\n        if len(search_results) &gt; 0:\n            return Catalog(search_results)\n        else:\n            raise Exception(\"No hits!\")\n</code></pre>"},{"location":"examples/catalog_example/","title":"Catalog example","text":"In\u00a0[1]: Copied! <pre>from gee_zonal import Catalog\n</pre> from gee_zonal import Catalog In\u00a0[2]: Copied! <pre>cat = Catalog()\n</pre> cat = Catalog() <p>Access the Earth Engine data catalog through Catalog.datasets</p> In\u00a0[3]: scroll-output Copied! <pre>cat.datasets.head()\n</pre> cat.datasets.head() Out[3]: id provider title start_date end_date startyear endyear type tags asset_url thumbnail_url 0 AAFC/ACI Agriculture and Agri-Food Canada Canada AAFC Annual Crop Inventory 2009-01-01 2020-01-01 2009 2020 image_collection [aafc, canada, crop, landcover] https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 1 ACA/reef_habitat/v1_0 Allen Coral Atlas Partnership (ACA) Allen Coral Atlas (ACA) - Geomorphic Zonation ... 2018-01-01 2021-01-01 2018 2021 image [coral, ocean, planet_derived, reef, seagrass,... https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 2 ACA/reef_habitat/v2_0 Allen Coral Atlas Partnership (ACA) Allen Coral Atlas (ACA) - Geomorphic Zonation ... 2018-01-01 2021-01-01 2018 2021 image [coral, ocean, planet_derived, reef, seagrass,... https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 3 AHN/AHN2_05M_INT AHN AHN Netherlands 0.5m DEM, Interpolated 2012-01-01 2012-01-01 2012 2012 image [ahn, dem, elevation, geophysical, lidar, neth... https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 4 AHN/AHN2_05M_NON AHN AHN Netherlands 0.5m DEM, Non-Interpolated 2012-01-01 2012-01-01 2012 2012 image [ahn, dem, elevation, geophysical, lidar, neth... https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... <p>The Catalog object also has search functionality. Search functions return a new catalog of datasets that matched the search parameters.</p> In\u00a0[4]: Copied! <pre>results = cat.search_tags(\"ndvi\")\n</pre> results = cat.search_tags(\"ndvi\") In\u00a0[5]: scroll-output Copied! <pre>print(results)\n</pre> print(results) <pre>251    Landsat 8 Collection 1 Tier 1 8-Day NDVI Compo...\n259    Landsat 8 Collection 1 Tier 1 32-Day NDVI Comp...\n268    Landsat 8 Collection 1 Tier 1 Annual NDVI Comp...\n292          Landsat 8 8-Day NDVI Composite [deprecated]\n300         Landsat 8 32-Day NDVI Composite [deprecated]\n309         Landsat 8 Annual NDVI Composite [deprecated]\n326    Landsat 7 Collection 1 Tier 1 8-Day NDVI Compo...\n334    Landsat 7 Collection 1 Tier 1 32-Day NDVI Comp...\n343    Landsat 7 Collection 1 Tier 1 Annual NDVI Comp...\n367          Landsat 7 8-Day NDVI Composite [deprecated]\n375         Landsat 7 32-Day NDVI Composite [deprecated]\n384         Landsat 7 Annual NDVI Composite [deprecated]\n431    Landsat 4 TM Collection 1 Tier 1 8-Day NDVI Co...\n439    Landsat 4 TM Collection 1 Tier 1 32-Day NDVI C...\n448    Landsat 4 TM Collection 1 Tier 1 Annual NDVI C...\n468       Landsat 4 TM 8-Day NDVI Composite [deprecated]\n476      Landsat 4 TM 32-Day NDVI Composite [deprecated]\n485      Landsat 4 TM Annual NDVI Composite [deprecated]\n496    Landsat 5 TM Collection 1 Tier 1 8-Day NDVI Co...\n504    Landsat 5 TM Collection 1 Tier 1 32-Day NDVI C...\n513    Landsat 5 TM Collection 1 Tier 1 Annual NDVI C...\n533       Landsat 5 TM 8-Day NDVI Composite [deprecated]\n541      Landsat 5 TM 32-Day NDVI Composite [deprecated]\n550      Landsat 5 TM Annual NDVI Composite [deprecated]\n588    MOD13A1.006 Terra Vegetation Indices 16-Day Gl...\n589    MOD13A2.006 Terra Vegetation Indices 16-Day Gl...\n590    MOD13Q1.006 Terra Vegetation Indices 16-Day Gl...\n609    MYD13A1.006 Aqua Vegetation Indices 16-Day Glo...\n610    MYD13A2.006 Aqua Vegetation Indices 16-Day Glo...\n611    MYD13Q1.006 Aqua Vegetation Indices 16-Day Glo...\n638    MOD13A1.061 Terra Vegetation Indices 16-Day Gl...\n639    MOD13A2.061 Terra Vegetation Indices 16-Day Gl...\n640    MOD13Q1.061 Terra Vegetation Indices 16-Day Gl...\n657    MYD13A1.061 Aqua Vegetation Indices 16-Day Glo...\n658    MYD13A2.061 Aqua Vegetation Indices 16-Day Glo...\n659    MYD13Q1.061 Aqua Vegetation Indices 16-Day Glo...\n675                           MODIS Combined 16-Day NDVI\n680              MODIS Combined 16-Day NDVI [deprecated]\n687                               MODIS Terra Daily NDVI\n692                  MODIS Terra Daily NDVI [deprecated]\n699    MOD13A1.005 Vegetation Indices 16-Day L3 Globa...\n700    MOD13Q1.005 Vegetation Indices 16-Day Global 2...\n707                                MODIS Aqua Daily NDVI\n712                   MODIS Aqua Daily NDVI [deprecated]\n719    MYD13A1.005 Vegetation Indices 16-Day L3 Globa...\n720    MYD13Q1.005 Vegetation Indices 16-Day Global 2...\n722    AG100: ASTER Global Emissivity Dataset 100-met...\n727       GIMMS NDVI From AVHRR Sensors (3rd Generation)\n768    NOAA CDR AVHRR NDVI: Normalized Difference Veg...\n769    NOAA CDR AVHRR NDVI: Normalized Difference Veg...\n807        VNP13A1: VIIRS Vegetation Indices 16-Day 500m\n811    VNP22Q2: Land Surface Phenology Yearly L3 Glob...\n</pre> In\u00a0[6]: Copied! <pre>results = results.search_by_year(1995)\n</pre> results = results.search_by_year(1995) In\u00a0[7]: scroll-output Copied! <pre>print(results)\n</pre> print(results) <pre>496    Landsat 5 TM Collection 1 Tier 1 8-Day NDVI Co...\n504    Landsat 5 TM Collection 1 Tier 1 32-Day NDVI C...\n513    Landsat 5 TM Collection 1 Tier 1 Annual NDVI C...\n533       Landsat 5 TM 8-Day NDVI Composite [deprecated]\n541      Landsat 5 TM 32-Day NDVI Composite [deprecated]\n550      Landsat 5 TM Annual NDVI Composite [deprecated]\n727       GIMMS NDVI From AVHRR Sensors (3rd Generation)\n768    NOAA CDR AVHRR NDVI: Normalized Difference Veg...\n769    NOAA CDR AVHRR NDVI: Normalized Difference Veg...\n</pre> In\u00a0[8]: Copied! <pre>results = results.search_title(\"landsat\")\n</pre> results = results.search_title(\"landsat\") In\u00a0[9]: scroll-output Copied! <pre>results.datasets\n</pre> results.datasets Out[9]: id provider title start_date end_date startyear endyear type tags asset_url thumbnail_url 496 LANDSAT/LT05/C01/T1_8DAY_NDVI Google Landsat 5 TM Collection 1 Tier 1 8-Day NDVI Co... 1984-01-01 2012-04-30 1984 2012 image_collection [landsat, ndvi, usgs] https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 504 LANDSAT/LT05/C01/T1_32DAY_NDVI Google Landsat 5 TM Collection 1 Tier 1 32-Day NDVI C... 1984-01-01 2012-04-06 1984 2012 image_collection [landsat, ndvi, usgs] https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 513 LANDSAT/LT05/C01/T1_ANNUAL_NDVI Google Landsat 5 TM Collection 1 Tier 1 Annual NDVI C... 1984-01-01 2012-01-01 1984 2012 image_collection [landsat, ndvi, usgs] https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 533 LANDSAT/LT5_L1T_8DAY_NDVI USGS Landsat 5 TM 8-Day NDVI Composite [deprecated] 1984-01-01 2012-04-30 1984 2012 image_collection [landsat, ndvi, usgs] https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 541 LANDSAT/LT5_L1T_32DAY_NDVI USGS Landsat 5 TM 32-Day NDVI Composite [deprecated] 1984-01-01 2012-04-06 1984 2012 image_collection [landsat, ndvi, usgs] https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat... 550 LANDSAT/LT5_L1T_ANNUAL_NDVI USGS Landsat 5 TM Annual NDVI Composite [deprecated] 1984-01-01 2012-01-01 1984 2012 image_collection [landsat, ndvi, usgs] https://developers.google.com/earth-engine/dat... https://developers.google.com/earth-engine/dat..."},{"location":"examples/catalog_example/#catalog-example","title":"Catalog Example\u00b6","text":""},{"location":"examples/zonal_statistics_example/","title":"Zonal statistics example","text":"In\u00a0[1]: Copied! <pre>from gee_zonal import ZonalStats\nimport ee\nimport geopandas as gpd\nimport pandas as pd\n</pre> from gee_zonal import ZonalStats import ee import geopandas as gpd import pandas as pd <p>Input target features can be referenced directly as a GEE asset, or can be supplied as a <code>geopandas.GeoDataFrame</code>, or a path to a shapefile/GeoJSON (will be automatically converted to <code>ee.FeatureCollection</code>).</p> In\u00a0[\u00a0]: Copied! <pre># !pip install pyarrow fsspec s3fs matplotlib folium mapclassify\n</pre> # !pip install pyarrow fsspec s3fs matplotlib folium mapclassify In\u00a0[3]: Copied! <pre>ftw = \"s3://us-west-2.opendata.source.coop/kerner-lab/fields-of-the-world-kenya/boundaries_kenya_2022.parquet\"\ngdf = gpd.read_parquet(ftw, storage_options={\"anon\": True})\n</pre> ftw = \"s3://us-west-2.opendata.source.coop/kerner-lab/fields-of-the-world-kenya/boundaries_kenya_2022.parquet\" gdf = gpd.read_parquet(ftw, storage_options={\"anon\": True}) In\u00a0[7]: Copied! <pre>gdf.head(50).explore()\n</pre> gdf.head(50).explore() Out[7]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[8]: Copied! <pre>AOIs = gdf.head(50)\n</pre> AOIs = gdf.head(50) In\u00a0[1]: Copied! <pre># ZonalStats?\n</pre> # ZonalStats? In\u00a0[10]: Copied! <pre>zs = ZonalStats(\n    target_features=AOIs,\n    statistic_type=\"mean\",\n    collection_id=\"MODIS/061/MYD13Q1\",\n    band=\"EVI\",\n    frequency=\"original\",\n    scale=250,\n    start_year=2023,\n    end_year=2025,\n    scale_factor=0.0001,\n)\n</pre> zs = ZonalStats(     target_features=AOIs,     statistic_type=\"mean\",     collection_id=\"MODIS/061/MYD13Q1\",     band=\"EVI\",     frequency=\"original\",     scale=250,     start_year=2023,     end_year=2025,     scale_factor=0.0001, ) In\u00a0[11]: scroll-output Copied! <pre>res = zs.runZonalStats()\nres.head()\n</pre> res = zs.runZonalStats() res.head() Out[11]: 2023_01_09_EVI 2023_01_25_EVI 2023_02_10_EVI 2023_02_26_EVI 2023_03_14_EVI 2023_03_30_EVI 2023_04_15_EVI 2023_05_01_EVI 2023_05_17_EVI 2023_06_02_EVI ... 2024_12_10_EVI 2024_12_26_EVI 2025_01_09_EVI 2025_01_25_EVI 2025_02_10_EVI 2025_02_26_EVI 2025_03_14_EVI crop_name determination_datetime id 0 0.279700 0.248700 0.1906 0.151100 0.158600 0.263800 0.449600 0.465700 0.402900 0.415700 ... 0.320000 0.353500 0.320500 0.223500 0.224000 0.216600 0.223200 Maize {'type': 'Date', 'value': 1646006400000} 322cb629-8921-4510-bf7f-70e99882f0a8 1 0.328100 0.294500 0.2524 0.215400 0.244500 0.355600 0.426000 0.504200 0.444800 0.367300 ... 0.368800 0.400700 0.419900 0.376100 0.349900 0.269700 0.253600 Sorghum {'type': 'Date', 'value': 1646006400000} 097d2b8a-7199-41db-8fb1-11e7fd76ea96 2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN Maize {'type': 'Date', 'value': 1646006400000} 0f398824-d43e-4d1e-8f78-7e21ba37dc89 3 0.281594 0.210688 0.1672 0.152165 0.174276 0.352435 0.407065 0.521312 0.477376 0.466771 ... 0.370306 0.344371 0.241012 0.216294 0.188129 0.172706 0.249918 Maize {'type': 'Date', 'value': 1646006400000} 721f1ae6-bdb9-4ef4-84de-1496dcb3a243 4 0.317300 0.256300 0.2371 0.248600 0.244400 0.312100 0.413200 0.552100 0.405900 0.389000 ... 0.324700 0.355600 0.394600 0.345900 0.349500 0.249400 0.277800 Sorghum {'type': 'Date', 'value': 1646006400000} beb30004-2952-4784-8590-ffb61e79658a <p>5 rows \u00d7 54 columns</p> In\u00a0[12]: Copied! <pre>def rename_func(col, var_name):\n    if var_name in col:\n        p = col.split(\"_\")\n        new_name = p[3] + \"_\" + p[0] + p[1] + p[2]\n        return new_name\n    else:\n        return col\n\n\ndef process_ts(df, var_name):\n    df.columns = df.columns.str.lower()\n    df.columns = df.apply(lambda x: rename_func(x.name, var_name), axis=0)\n    df_re = pd.wide_to_long(df, stubnames=[var_name], i=\"id\", j=\"date\", sep=\"_\")\n    df_re.reset_index(inplace=True)\n    return df_re\n</pre> def rename_func(col, var_name):     if var_name in col:         p = col.split(\"_\")         new_name = p[3] + \"_\" + p[0] + p[1] + p[2]         return new_name     else:         return col   def process_ts(df, var_name):     df.columns = df.columns.str.lower()     df.columns = df.apply(lambda x: rename_func(x.name, var_name), axis=0)     df_re = pd.wide_to_long(df, stubnames=[var_name], i=\"id\", j=\"date\", sep=\"_\")     df_re.reset_index(inplace=True)     return df_re In\u00a0[13]: Copied! <pre>df_re = process_ts(res, \"evi\")\ndf_re[\"date\"] = pd.to_datetime(df_re[\"date\"], format=\"%Y%m%d\")\ndf_re.dropna(subset=[\"evi\"], inplace=True)\ndf_re[\"evi\"] = df_re[\"evi\"].astype(\"float\")\n</pre> df_re = process_ts(res, \"evi\") df_re[\"date\"] = pd.to_datetime(df_re[\"date\"], format=\"%Y%m%d\") df_re.dropna(subset=[\"evi\"], inplace=True) df_re[\"evi\"] = df_re[\"evi\"].astype(\"float\") In\u00a0[14]: Copied! <pre>df_re.head()\n</pre> df_re.head() Out[14]: id date crop_name determination_datetime evi 0 322cb629-8921-4510-bf7f-70e99882f0a8 2023-01-09 Maize {'type': 'Date', 'value': 1646006400000} 0.279700 1 097d2b8a-7199-41db-8fb1-11e7fd76ea96 2023-01-09 Sorghum {'type': 'Date', 'value': 1646006400000} 0.328100 3 721f1ae6-bdb9-4ef4-84de-1496dcb3a243 2023-01-09 Maize {'type': 'Date', 'value': 1646006400000} 0.281594 4 beb30004-2952-4784-8590-ffb61e79658a 2023-01-09 Sorghum {'type': 'Date', 'value': 1646006400000} 0.317300 5 9b5056ad-e781-4625-98f0-6b94aaa15f87 2023-01-09 Maize {'type': 'Date', 'value': 1646006400000} 0.296800 In\u00a0[15]: Copied! <pre># !pip install plotnine\n</pre> # !pip install plotnine In\u00a0[16]: Copied! <pre>from plotnine import ggplot, aes, geom_line, labs, scale_x_datetime, theme, theme_bw\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\n</pre> from plotnine import ggplot, aes, geom_line, labs, scale_x_datetime, theme, theme_bw from mizani.breaks import date_breaks from mizani.formatters import date_format In\u00a0[17]: Copied! <pre>df_re.dropna(subset=[\"evi\"], inplace=True)\ndf_re[\"evi\"] = df_re[\"evi\"].astype(\"float\")\n</pre> df_re.dropna(subset=[\"evi\"], inplace=True) df_re[\"evi\"] = df_re[\"evi\"].astype(\"float\") In\u00a0[22]: Copied! <pre>(\n    ggplot(df_re, aes(x=\"date\", y=\"evi\", group=\"id\"))\n    + geom_line(alpha=0.20, color=\"teal\")\n    + labs(x=\"\", y=\"EVI\", title=\"Enhanced Vegetation Index per Field\")\n    + scale_x_datetime(\n        breaks=date_breaks(width=\"6 months\"), labels=date_format(\"%Y, %m\")\n    )\n    + theme(figure_size=(10, 6), legend_position=\"none\")\n    + theme_bw()\n    # + scale_color_brewer(palette='Greens')\n)\n</pre> (     ggplot(df_re, aes(x=\"date\", y=\"evi\", group=\"id\"))     + geom_line(alpha=0.20, color=\"teal\")     + labs(x=\"\", y=\"EVI\", title=\"Enhanced Vegetation Index per Field\")     + scale_x_datetime(         breaks=date_breaks(width=\"6 months\"), labels=date_format(\"%Y, %m\")     )     + theme(figure_size=(10, 6), legend_position=\"none\")     + theme_bw()     # + scale_color_brewer(palette='Greens') ) In\u00a0[2]: Copied! <pre>from gee_zonal import authenticateGoogleDrive\n</pre> from gee_zonal import authenticateGoogleDrive <p>By default, <code>authenticateGoogleDrive</code> looks for client credentials under <code>~/.config/creds</code>.</p> <p>You can change this path by setting the parameter <code>creds_dir</code> to the function: <code>authenticateGoogleDrive(creds_dir=\"path/to/directory\")</code>.</p> In\u00a0[3]: Copied! <pre>drive = authenticateGoogleDrive()\n</pre> drive = authenticateGoogleDrive() In\u00a0[28]: Copied! <pre>gaul_adm1 = ee.FeatureCollection(\"projects/sat-io/open-datasets/FAO/GAUL/GAUL_2024_L1\")\n</pre> gaul_adm1 = ee.FeatureCollection(\"projects/sat-io/open-datasets/FAO/GAUL/GAUL_2024_L1\") In\u00a0[29]: Copied! <pre># AOIs = gaul_adm1.filterMetadata('gaul0_name', 'equals', 'Kenya')\nAOIs_AFR = gaul_adm1.filterMetadata(\"continent\", \"equals\", \"Africa\")\n</pre> # AOIs = gaul_adm1.filterMetadata('gaul0_name', 'equals', 'Kenya') AOIs_AFR = gaul_adm1.filterMetadata(\"continent\", \"equals\", \"Africa\") In\u00a0[30]: Copied! <pre>zs = ZonalStats(\n    collection_id=\"UCSB-CHG/CHIRPS/PENTAD\",\n    target_features=AOIs_AFR,\n    statistic_type=\"mean\",\n    frequency=\"annual\",\n    temporal_stat=\"sum\",\n    scale=5000,\n    output_dir=\"africa\",\n    output_name=\"precipitation\",\n)\n</pre> zs = ZonalStats(     collection_id=\"UCSB-CHG/CHIRPS/PENTAD\",     target_features=AOIs_AFR,     statistic_type=\"mean\",     frequency=\"annual\",     temporal_stat=\"sum\",     scale=5000,     output_dir=\"africa\",     output_name=\"precipitation\", ) In\u00a0[31]: Copied! <pre>zs.runZonalStats()\n</pre> zs.runZonalStats() In\u00a0[34]: Copied! <pre>zs.reportRunTime()\n</pre> zs.reportRunTime() <pre>Completed\nRuntime: 0 minutes and 25 seconds\n</pre> In\u00a0[35]: Copied! <pre>df = zs.getZonalStats(drive)\n</pre> df = zs.getZonalStats(drive) In\u00a0[36]: Copied! <pre>df.head()\n</pre> df.head() Out[36]: 1981_precipitation_sum 1982_precipitation_sum 1983_precipitation_sum 1984_precipitation_sum 1985_precipitation_sum 1986_precipitation_sum 1987_precipitation_sum 1988_precipitation_sum 1989_precipitation_sum 1990_precipitation_sum ... 2024_precipitation_sum 2025_precipitation_sum continent disp_en gaul0_code gaul0_name gaul1_code gaul1_name iso3_code map_code 0 1269.434847 1943.335051 1630.475669 1412.186651 1978.640470 1396.402007 1707.969199 1869.594519 1526.631987 1924.089518 ... 2885.706123 414.657800 Africa Praslin, Seychelles 156 Seychelles 1583 Praslin SYC SYC 1 1340.485273 1414.347985 1116.546663 1249.549483 1291.161982 1295.594058 1299.800270 1385.350324 1374.347415 1273.910652 ... 1341.073959 88.210590 Africa Central, Uganda 165 Uganda 1675 Central UGA UGA 2 1286.473853 1336.082890 1188.696247 1076.145691 1321.348013 1120.096568 1214.078093 1474.526842 1352.004762 1375.142717 ... 1520.903469 55.699340 Africa Eastern, Uganda 165 Uganda 1676 Eastern UGA UGA 3 1138.579648 1185.657568 1109.306795 925.817103 1155.325492 1018.788433 1132.352481 1249.080423 1196.329453 1225.392959 ... 1217.854720 13.953354 Africa Northern, Uganda 165 Uganda 1677 Northern UGA UGA 4 1196.080492 1171.360128 1061.412981 1050.639443 1144.941604 1107.395590 1103.530371 1206.420453 1127.987972 1104.468894 ... 1102.383293 62.125268 Africa Western, Uganda 165 Uganda 1678 Western UGA UGA <p>5 rows \u00d7 53 columns</p> In\u00a0[37]: Copied! <pre>len(df)\n</pre> len(df) Out[37]: <pre>731</pre>"},{"location":"examples/zonal_statistics_example/#notebook-example","title":"Notebook Example\u00b6","text":""},{"location":"examples/zonal_statistics_example/#input-features","title":"Input Features\u00b6","text":""},{"location":"examples/zonal_statistics_example/#initialize-zonal-stats","title":"Initialize Zonal Stats\u00b6","text":""},{"location":"examples/zonal_statistics_example/#option-a-retrieve-results-directly","title":"Option A: Retrieve results directly\u00b6","text":"<p>Output: DataFrame with statistics Recommended for small areas / low n</p>"},{"location":"examples/zonal_statistics_example/#option-b-submit-a-task","title":"Option B: Submit a task\u00b6","text":"<p>Output: csv table on Google Drive Recommended for big areas / high n</p>"}]}